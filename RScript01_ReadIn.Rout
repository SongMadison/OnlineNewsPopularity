
R version 3.2.2 (2015-08-14) -- "Fire Safety"
Copyright (C) 2015 The R Foundation for Statistical Computing
Platform: x86_64-redhat-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> 
> rm(list =ls())
> 
> RScriptPath <- paste0("./RScripts/")
> PlotsPath <- paste0("./Plots/")
> DataPath <- paste0("./Data/")
> 
> source("./RScripts/fn_Library.R")
> library(AUC)
AUC 0.3.0
Type AUCNews() to see the change log and ?AUC to get an overview.
> 
> news = read.csv(file = paste0(DataPath,"OnlineNewsPopularity.csv"), header = TRUE)
> trainIdx <- sample(1:nrow(news), 30000) # 30000
> testIdx <- (1:nrow(news))[-trainIdx]  # 9644
> train <- news[trainIdx,]
> test <- news[testIdx,]
> 
> 
> # combining some the factor variable into a factor variable 
> ## Caution !! some methods doesn't accept factor variable.  Using the given coding for
> #  factor variable is good in this sense!!
> 
> # train <- PrepareData(train)
> # test <- PrepareData(test)
> # 
> # write.csv(train, paste0(DataPath,"train.csv"))
> # write.csv(test, paste0(DataPath,"test.csv"))
> 
> 
> 
> train.classification <- train[,-1]
> test.classification <- test[,-1]
> train.classification$shares <- ( train.classification$shares>1400)*1.0 
> test.classification$shares <-  ( test.classification$shares>1400)*1.0 
> (table(train.classification$shares)/nrow(train.classification))

        0         1 
0.5075667 0.4924333 
> (table(test.classification$shares)/nrow(test.classification))

        0         1 
0.5034218 0.4965782 
> 
> 
> dim(train.classification)
[1] 30000    60
> dim(test.classification)
[1] 9644   60
> x.tr <- train.classification[-ncol(train.classification)]
> y.tr <- train.classification[,ncol(train.classification)]
> x.te <- test.classification[-ncol(test.classification)]
> y.te <- test.classification[,ncol(test.classification)]
> rm(train, test, news)
> 
> ####################################################################################
> ## Logistic Regression 
> ####################################################################################
> fit.glm <- glm(as.factor(train.classification$shares) ~ . ,
+                    family = binomial(link = "logit"), data =train.classification)
> 
> predicted.tr <- (predict(fit.glm, type="response")>0.5)*1
> hist(predict(fit.glm,type="link")) # log (odd ratio)
> (confusionM.tr <- table(y.tr, predicted.tr))
    predicted.tr
y.tr     0     1
   0 10328  4899
   1  5529  9244
> mean(y.tr==predicted.tr)
[1] 0.6524
> 
> mean(y.te==(predict(fit.glm,newdata = x.te,type="response")>0.5))
[1] 0.6529448
Warning message:
In predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type ==  :
  prediction from a rank-deficient fit may be misleading
> 
> 
> plot(roc.logit.te <- roc(predict(fit.glm,x.te,type = "response") ,as.factor(y.te)))
Warning message:
In predict.lm(object, newdata, se.fit, scale = 1, type = ifelse(type ==  :
  prediction from a rank-deficient fit may be misleading
> auc(roc.logit.te)
[1] 0.705152
> 
> 
> plot(roc.logit.tr <- roc(fit.glm$fitted,as.factor(y.tr)))
> auc(roc.logit.tr)
[1] 0.7031226
> 
> #####################################################################################
> ## Fit LASSO-logistic regression
> #####################################################################################
> ## Select lambda using 10-fold CV
> library(glmnet)
Loading required package: Matrix
Loading required package: foreach
Loaded glmnet 2.0-2


Attaching package: ‘glmnet’

The following object is masked from ‘package:AUC’:

    auc

> 
> cv.lasso = cv.glmnet(as.matrix(x.tr), as.matrix(y.tr), family='binomial', nfolds=10)
> lambda.min = cv.lasso$lambda.min
> lambda.1se = cv.lasso$lambda.1se ## Result in a sparser model
> 
> ## Fit the final model with lambda.min
> fit.min = glmnet(as.matrix(x.tr), as.matrix(y.tr), family='binomial', lambda=lambda.min)
> fit.min$a0 ## intercept
       s0 
-1.920536 
> fit.min$beta ## betahat in logistic regression
59 x 1 sparse Matrix of class "dgCMatrix"
                                         s0
timedelta                      8.016730e-05
n_tokens_title                 4.160662e-03
n_tokens_content               1.758501e-04
n_unique_tokens               -7.449197e-02
n_non_stop_words               .           
n_non_stop_unique_tokens      -5.115861e-01
num_hrefs                      8.267226e-03
num_self_hrefs                -2.172924e-02
num_imgs                       1.222530e-03
num_videos                     2.331800e-04
average_token_length          -9.092738e-02
num_keywords                   3.114460e-02
data_channel_is_lifestyle     -1.761390e-01
data_channel_is_entertainment -3.009469e-01
data_channel_is_bus           -3.048342e-01
data_channel_is_socmed         7.145801e-01
data_channel_is_tech           4.194785e-01
data_channel_is_world         -1.140873e-02
kw_min_min                     1.465480e-03
kw_max_min                     3.565937e-05
kw_avg_min                    -2.801915e-04
kw_min_max                    -5.861567e-07
kw_max_max                    -3.895685e-07
kw_avg_max                    -5.479794e-07
kw_min_avg                    -8.286840e-05
kw_max_avg                    -8.263449e-05
kw_avg_avg                     6.889359e-04
self_reference_min_shares      2.791334e-06
self_reference_max_shares      5.241726e-07
self_reference_avg_sharess     9.952451e-07
weekday_is_monday              5.473490e-03
weekday_is_tuesday            -1.194938e-01
weekday_is_wednesday          -1.289705e-01
weekday_is_thursday           -6.639526e-02
weekday_is_friday              1.174391e-01
weekday_is_saturday            7.039694e-01
weekday_is_sunday              4.431520e-01
is_weekend                     2.257482e-01
LDA_00                         1.063690e+00
LDA_01                        -5.869075e-02
LDA_02                        -1.802574e-01
LDA_03                        -4.717836e-02
LDA_04                         5.383836e-01
global_subjectivity            9.712132e-01
global_sentiment_polarity      1.605214e-02
global_rate_positive_words    -2.042274e+00
global_rate_negative_words     3.682595e+00
rate_positive_words            4.632178e-01
rate_negative_words            .           
avg_positive_polarity         -3.581742e-01
min_positive_polarity         -3.167811e-01
max_positive_polarity          1.875751e-02
avg_negative_polarity          .           
min_negative_polarity          .           
max_negative_polarity         -1.704050e-01
title_subjectivity             1.481386e-01
title_sentiment_polarity       1.978593e-01
abs_title_subjectivity         3.288038e-01
abs_title_sentiment_polarity   .           
> pred.min = predict(fit.min, as.matrix(x.te), type='class')
> 
> mean(pred.min!=y.te)
[1] 0.3479884
> 
> ## Fit the final model with lambda.1se
> fit.1se = glmnet(as.matrix(x.tr), as.matrix(y.tr), family='binomial', lambda=lambda.1se)
> fit.1se$a0 ## intercept
       s0 
-1.628106 
> fit.1se$beta ## betahat in logistic regression
59 x 1 sparse Matrix of class "dgCMatrix"
                                         s0
timedelta                      1.098646e-05
n_tokens_title                 .           
n_tokens_content               1.216664e-04
n_unique_tokens               -2.127902e-01
n_non_stop_words               .           
n_non_stop_unique_tokens      -4.128924e-01
num_hrefs                      7.123230e-03
num_self_hrefs                -1.408343e-02
num_imgs                       5.928048e-04
num_videos                     .           
average_token_length          -7.708850e-03
num_keywords                   3.560850e-02
data_channel_is_lifestyle      .           
data_channel_is_entertainment -3.121707e-01
data_channel_is_bus           -1.550882e-01
data_channel_is_socmed         7.612222e-01
data_channel_is_tech           4.713816e-01
data_channel_is_world          .           
kw_min_min                     1.381733e-03
kw_max_min                     .           
kw_avg_min                    -2.305789e-07
kw_min_max                    -5.276793e-07
kw_max_max                    -2.906387e-07
kw_avg_max                    -1.763266e-07
kw_min_avg                    -1.373025e-05
kw_max_avg                    -5.196276e-05
kw_avg_avg                     4.914989e-04
self_reference_min_shares      1.477296e-06
self_reference_max_shares      .           
self_reference_avg_sharess     1.788695e-06
weekday_is_monday              1.895646e-02
weekday_is_tuesday            -4.591094e-02
weekday_is_wednesday          -5.924546e-02
weekday_is_thursday            .           
weekday_is_friday              1.355158e-01
weekday_is_saturday            2.276633e-01
weekday_is_sunday              .           
is_weekend                     6.847464e-01
LDA_00                         6.914381e-01
LDA_01                        -6.222020e-02
LDA_02                        -3.496419e-01
LDA_03                         .           
LDA_04                         3.018150e-01
global_subjectivity            7.831819e-01
global_sentiment_polarity      .           
global_rate_positive_words     .           
global_rate_negative_words     .           
rate_positive_words            .           
rate_negative_words           -1.184739e-01
avg_positive_polarity         -1.200983e-02
min_positive_polarity         -3.225203e-01
max_positive_polarity          .           
avg_negative_polarity          .           
min_negative_polarity          .           
max_negative_polarity         -1.198005e-02
title_subjectivity             9.321439e-02
title_sentiment_polarity       1.469215e-01
abs_title_subjectivity         1.921486e-01
abs_title_sentiment_polarity   .           
> pred.1se = predict(fit.1se, as.matrix(x.te), type='class')
> mean(pred.1se!=y.te)
[1] 0.3494401
> 
> ####################################################################################
> ## random Forest
> ####################################################################################
> library(randomForest)
randomForest 4.6-12
Type rfNews() to see new features/changes/bug fixes.
> ## Try different mtryStart values manually
> set.seed(111)
> tune.rf = tuneRF(x=train.classification[-ncol(train.classification)], 
+                  y=train.classification[,ncol(train.classification)], ntree=1000, mtryStart=4, stepFactor=2,
+                  improve=0.05, nodesize=1, trace=T, plot=T,doBest=T)
mtry = 4  OOB error = 0.2120014 
Searching left ...
mtry = 2 	OOB error = 0.2133305 
-0.006268882 0.05 
Searching right ...
